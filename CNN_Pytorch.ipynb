{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01786b3-7f37-4b62-a04d-1ec0a308aea9",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcd4d1a-b1fd-4a9a-a083-895d7a22bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "Rebuild_Data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284794a9-0ce8-40b8-8b9a-23b2bd5dea2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e9f1c0-6c18-4462-b949-15253047af13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DogsVSCats:\n",
    "    IMG_SIZE = (50, 50)\n",
    "    CATS = \"../PetImages/Cat\"\n",
    "    DOGS = \"../PetImages/Dog\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_data = []\n",
    "        self.catcount = 0\n",
    "        self.dogcount = 0\n",
    "        self.hg_data = []\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                path = os.path.join(label, f)\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                one_hot_label = np.eye(2)[self.LABELS[label]]\n",
    "               \n",
    "\n",
    "                if img is not None:\n",
    "                    try:\n",
    "                        img = cv2.resize(img, self.IMG_SIZE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to process {f}: {e}\")\n",
    "                        continue  # Skip this image if an error occurs\n",
    "                    self.training_data.append(np.array([np.array(img), one_hot_label],dtype=\"object\"))\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount += 1\n",
    "\n",
    "        for data in tqdm(self.training_data):\n",
    "            if (len(data) == 2 and isinstance(data[0], np.ndarray) and data[0].shape == self.IMG_SIZE and isinstance(data[1], np.ndarray) and data[1].shape == (2,)):\n",
    "                self.hg_data.append(data)\n",
    "            else:\n",
    "                print(f\"Skipping inhomogeneous element: {data}\")\n",
    "\n",
    "        # Save the homogeneous data with allow_pickle=True\n",
    "        np.save(\"../training_data.npy\",self.hg_data, allow_pickle=True)\n",
    "        print(\"Cats:\", self.catcount)\n",
    "        print(\"Dogs:\", self.dogcount)\n",
    "\n",
    "# Example usage\n",
    "Rebuild_Data = False\n",
    "if Rebuild_Data:\n",
    "    dataset = DogsVSCats()\n",
    "    dataset.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e86f68d-35d1-44c1-be2e-354b661b4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"../training_data.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a1c759-92f5-4bc7-bae1-7827aec7fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5d7f35-5f6e-4853-a31b-65fd48a0d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 44,  60,  41, ...,  95,  94, 197],\n",
      "        [ 41,  43,  43, ...,  93,  80, 192],\n",
      "        [ 41,  40,  46, ...,  90,  86, 193],\n",
      "        ...,\n",
      "        [ 25,  21,  26, ...,  43,  67,  73],\n",
      "        [ 21,  23,  21, ...,  74,  36,  89],\n",
      "        [ 23,  22,  20, ...,  59,  62,  32]], dtype=uint8) array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b60446-a640-45a3-a404-97bc8f7c8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(training_data[11][0],cmap=\"gray\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd56a-eaa4-4fdf-a612-6f89894313e2",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97facfe7-1655-4eb7-9fdd-2a9a5602a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 50, 50])\n",
      "After conv1: torch.Size([1, 32, 46, 46])\n",
      "After pool1: torch.Size([1, 32, 23, 23])\n",
      "After conv2: torch.Size([1, 64, 19, 19])\n",
      "After pool2: torch.Size([1, 64, 9, 9])\n",
      "After conv3: torch.Size([1, 128, 5, 5])\n",
      "After pool3: torch.Size([1, 128, 2, 2])\n",
      "After flatten: torch.Size([1, 512])\n",
      "After fc1: torch.Size([1, 16])\n",
      "After fc2: torch.Size([1, 2])\n",
      "Output: tensor([[0.1466, 0.0320]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 1)  # 5x5 kernel, 1 input channel, 32 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1) # 5x5 kernel, 32 input channels, 64 output channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 1) # 5x5 kernel, 64 input channels, 128 output channels\n",
    "        # Calculate the flattened size after conv and pooling layers.\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 16)  # Update the input size accordingly\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(\"After conv1:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool1:\", x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(\"After conv2:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool2:\", x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(\"After conv3:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool3:\", x.shape)\n",
    "        x = x.view(-1, 128 * 2 * 2)  # Flatten the tensor\n",
    "        print(\"After flatten:\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"After fc1:\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"After fc2:\", x.shape)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "net = Net()\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 1, 50, 50)  # Batch size of 1, 1 channel, 50x50 image\n",
    "\n",
    "output = net.forward(dummy_input)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447683ad-da37-40d9-8158-6062b1e4da24",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "# Hidden Layers\n",
    "\n",
    "**Role:**\n",
    "- The hidden layers transform the input data into a space where the patterns relevant to the problem can be more easily separated.\n",
    "- These layers create complex, non-linear mappings of the input data.\n",
    "\n",
    "## **Common Activation Functions:**\n",
    "## $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "  - Pros: Helps with the vanishing gradient problem, computationally efficient.\n",
    "  - Cons: Can suffer from the \"dying ReLU\" problem where neurons can get stuck during training.\n",
    "## $ \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} $\n",
    "  - Pros: Outputs values between 0 and 1, can be interpreted as probabilities.\n",
    "  - Cons: Can cause vanishing gradient problems.\n",
    "## $ \\text{Tanh}(x) =  \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "  - Pros: Outputs values between -1 and 1, zero-centered.\n",
    "  - Cons: Also suffers from vanishing gradient issues.\n",
    "\n",
    "# Output Layers\n",
    "\n",
    "**Role:**\n",
    "- The output layer produces the final result of the neural network, which could be a classification, a regression value, etc.\n",
    "- Different problems require different output ranges and interpretations.\n",
    "\n",
    "# **Common Activation Functions for Output Layers:**\n",
    "## **Softmax:** Typically used in multi-class classification problems.\n",
    "> ## $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $\n",
    "  - Pros: Converts logits into probabilities that sum to 1.\n",
    "  - Cons: Not suitable for regression tasks.\n",
    "## **Sigmoid:** Often used in binary classification problems.\n",
    "> ## $ \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} $\n",
    "  - Pros: Outputs values between 0 and 1.\n",
    "  - Cons: Limited to binary outcomes.\n",
    "## **Linear:** Used in regression problems.\n",
    "> ## $ \\text{Linear}(x) = x $\n",
    "  - Pros: Suitable for predicting continuous values.\n",
    "  - Cons: Not suitable for classification tasks.\n",
    "\n",
    "# Why Not Use ReLU in the Output Layer?\n",
    "\n",
    "**ReLU Characteristics:**\n",
    "- Outputs any positive value as itself and zero for any negative input.\n",
    "- Not bounded, meaning its output range is $[0, \\infty)$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "1. **Classification Problem:**\n",
    "   - If you use ReLU in the output layer for a classification problem (e.g., a multi-class classification task), you would get non-negative outputs but not probabilities. Classification typically requires interpreting the output as probabilities, which should sum to 1. ReLU cannot naturally enforce this constraint, unlike the Softmax function.\n",
    "\n",
    "2. **Regression Problem:**\n",
    "   - For regression tasks, using ReLU could limit the range of the output to non-negative values only. This might not be desirable if the target values can be negative. For example, predicting house prices could work with ReLU if prices are always positive, but predicting temperature, which can be negative, would be inappropriate with ReLU.\n",
    "\n",
    "### Detailed Example: Multi-Class Classification\n",
    "\n",
    "Consider a neural network designed to classify images into one of three categories: Cat, Dog, and Bird. The final layer must output a probability distribution over these three classes.\n",
    "\n",
    "**Using ReLU in the Output Layer:**\n",
    "- The network might output values like [2.5, 0, 3.7] for an input image.\n",
    "- These values are not probabilities and do not sum to 1.\n",
    "- There's no straightforward way to interpret these values as the likelihood of each class.\n",
    "\n",
    "**Using Softmax in the Output Layer:**\n",
    "- The network might output logits [2.5, 0, 3.7].\n",
    "- After applying Softmax, these logits might convert to probabilities like [0.28, 0.01, 0.71].\n",
    "- These probabilities sum to 1, making it clear that the model predicts \"Bird\" with 71% confidence.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The choice of activation function in the output layer is crucial and must align with the problem requirements. While ReLU is excellent for hidden layers due to its properties that help mitigate issues like the vanishing gradient problem, it is not suitable for output layers in many cases because it does not provide a bounded, interpretable output suitable for classification or regression tasks. Different tasks necessitate different activation functions to ensure the outputs are meaningful and useful for the specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56831888-46f8-4351-8d8e-aa5948188a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4510, 0.5490]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 1)  # 5x5 kernel, 1 input channel, 32 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1) # 5x5 kernel, 32 input channels, 64 output channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 1) # 5x5 kernel, 64 input channels, 128 output channels\n",
    "        # Calculate the flattened size after conv and pooling layers.\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 16)  # Update the input size accordingly\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128 * 2 * 2)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "\n",
    "# Example usage\n",
    "net = Net()\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 1, 50, 50)  # Batch size of 1, 1 channel, 50x50 image\n",
    "\n",
    "output = net.forward(dummy_input)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82cda75-482a-4a7f-90f7-cd9dfc017bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),0.0001)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f46b446-ba88-400f-807f-c946b879440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opdar\\AppData\\Local\\Temp\\ipykernel_13464\\3353633519.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ad9152-464c-47ad-9146-53cef3eaabc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae847a5-6298-4488-afa9-0d56b3ec1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "# train_X and train_y will contain data for training\n",
    "# test_X and test_y will contain data for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77ad59-8fc2-4da8-b6c7-51927cbebbe8",
   "metadata": {},
   "source": [
    "The main difference between `model.zero_grads()` and `optimizer.zero_grads()` lies in the scope of the operation:\r\n",
    "\r\n",
    "- `model.zero_grads()`: This method zeroes the gradients of all parameters in the model. It directly operates on the model itself, making it suitable for cases where you want to manually manage the gradient clearing process within the model.\r\n",
    "\r\n",
    "- `optimizer.zero_grads()`: On the other hand, this method zeroes the gradients of only the parameters that the optimizer is responsible for updating. It operates on the optimizer object and is commonly used during the training loop after the backward pass to clear gradients before the optimizer updates the model parametmized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f973efb3-d5e5-4d71-b989-167da8f9f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6275, 0.5882, 0.6275,  ..., 0.4078, 0.4118, 0.3765],\n",
      "          [0.6784, 0.6118, 0.6078,  ..., 0.3765, 0.3020, 0.2824],\n",
      "          [0.6824, 0.6588, 0.6196,  ..., 0.2471, 0.2980, 0.3647],\n",
      "          ...,\n",
      "          [0.9647, 0.9451, 0.9451,  ..., 0.7725, 0.7686, 0.7255],\n",
      "          [0.9529, 0.9373, 0.9373,  ..., 0.8235, 0.7686, 0.7294],\n",
      "          [0.9765, 0.9765, 0.9686,  ..., 0.8431, 0.9176, 0.7765]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 0.9922,  ..., 0.5255, 0.5529, 0.4980],\n",
      "          [0.9961, 0.9922, 0.9882,  ..., 0.3765, 0.5765, 0.5529],\n",
      "          [0.9922, 0.9961, 0.9961,  ..., 0.6314, 0.6196, 0.5255],\n",
      "          ...,\n",
      "          [0.4000, 0.5765, 0.5569,  ..., 0.7804, 0.7882, 0.7961],\n",
      "          [0.4510, 0.5765, 0.4902,  ..., 0.7412, 0.7333, 0.7569],\n",
      "          [0.4627, 0.5412, 0.4000,  ..., 0.7529, 0.7176, 0.4627]]],\n",
      "\n",
      "\n",
      "        [[[0.0275, 0.0353, 0.0314,  ..., 0.5922, 0.3804, 0.2000],\n",
      "          [0.0314, 0.0314, 0.0392,  ..., 0.6118, 0.4784, 0.2275],\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.6118, 0.5020, 0.2980],\n",
      "          ...,\n",
      "          [0.5176, 0.4941, 0.4275,  ..., 0.2745, 0.1961, 0.1961],\n",
      "          [0.5608, 0.5569, 0.5961,  ..., 0.2235, 0.2431, 0.2784],\n",
      "          [0.5451, 0.5373, 0.5412,  ..., 0.2000, 0.1765, 0.1529]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2667, 0.3098, 0.3216,  ..., 0.3608, 0.3294, 0.0863],\n",
      "          [0.2706, 0.3216, 0.3373,  ..., 0.3882, 0.3451, 0.0941],\n",
      "          [0.2902, 0.3255, 0.3490,  ..., 0.4039, 0.3686, 0.1137],\n",
      "          ...,\n",
      "          [0.3098, 0.3412, 0.3333,  ..., 0.4118, 0.3843, 0.1333],\n",
      "          [0.2863, 0.3216, 0.2980,  ..., 0.3882, 0.3569, 0.1255],\n",
      "          [0.2627, 0.2902, 0.2549,  ..., 0.3647, 0.3216, 0.1216]]],\n",
      "\n",
      "\n",
      "        [[[0.4314, 0.4510, 0.4784,  ..., 0.3294, 0.4314, 0.4000],\n",
      "          [0.4549, 0.4863, 0.4902,  ..., 0.4275, 0.4275, 0.4078],\n",
      "          [0.4745, 0.5176, 0.3647,  ..., 0.3569, 0.4196, 0.4353],\n",
      "          ...,\n",
      "          [0.4510, 0.4784, 0.5098,  ..., 0.5137, 0.4863, 0.4706],\n",
      "          [0.4510, 0.4745, 0.4980,  ..., 0.4941, 0.4902, 0.4706],\n",
      "          [0.4157, 0.4353, 0.4745,  ..., 0.5020, 0.4902, 0.4471]]],\n",
      "\n",
      "\n",
      "        [[[0.2863, 0.2824, 0.2902,  ..., 0.7569, 0.7922, 0.8392],\n",
      "          [0.2667, 0.2627, 0.2667,  ..., 0.6902, 0.7137, 0.7569],\n",
      "          [0.2510, 0.2431, 0.2431,  ..., 0.6275, 0.6314, 0.7020],\n",
      "          ...,\n",
      "          [0.0784, 0.0863, 0.0980,  ..., 0.4627, 0.5882, 0.5804],\n",
      "          [0.0706, 0.0784, 0.0745,  ..., 0.5098, 0.5961, 0.5608],\n",
      "          [0.0667, 0.0784, 0.0706,  ..., 0.5098, 0.6118, 0.3961]]]])\n",
      "tensor([[0.4459, 0.5541],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.4484, 0.5516],\n",
      "        [0.4464, 0.5536],\n",
      "        [0.4466, 0.5534],\n",
      "        [0.4472, 0.5528],\n",
      "        [0.4475, 0.5525],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4484, 0.5516],\n",
      "        [0.4480, 0.5520],\n",
      "        [0.4479, 0.5521],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.4473, 0.5527],\n",
      "        [0.4468, 0.5532],\n",
      "        [0.4462, 0.5538],\n",
      "        [0.4464, 0.5536],\n",
      "        [0.4465, 0.5535],\n",
      "        [0.4487, 0.5513],\n",
      "        [0.4469, 0.5531],\n",
      "        [0.4485, 0.5515],\n",
      "        [0.4478, 0.5522],\n",
      "        [0.4483, 0.5517],\n",
      "        [0.4449, 0.5551],\n",
      "        [0.4449, 0.5551],\n",
      "        [0.4474, 0.5526],\n",
      "        [0.4482, 0.5518],\n",
      "        [0.4491, 0.5509],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4483, 0.5517],\n",
      "        [0.4481, 0.5519],\n",
      "        [0.4468, 0.5532],\n",
      "        [0.4465, 0.5535],\n",
      "        [0.4475, 0.5525],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.4478, 0.5522],\n",
      "        [0.4481, 0.5519],\n",
      "        [0.4483, 0.5517],\n",
      "        [0.4462, 0.5538],\n",
      "        [0.4463, 0.5537],\n",
      "        [0.4473, 0.5527],\n",
      "        [0.4479, 0.5521],\n",
      "        [0.4473, 0.5527],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4494, 0.5506],\n",
      "        [0.4478, 0.5522],\n",
      "        [0.4482, 0.5518],\n",
      "        [0.4477, 0.5523],\n",
      "        [0.4487, 0.5513],\n",
      "        [0.4479, 0.5521],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.4483, 0.5517],\n",
      "        [0.4456, 0.5544],\n",
      "        [0.4484, 0.5516],\n",
      "        [0.4488, 0.5512],\n",
      "        [0.4481, 0.5519],\n",
      "        [0.4468, 0.5532],\n",
      "        [0.4468, 0.5532],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.4475, 0.5525],\n",
      "        [0.4482, 0.5518],\n",
      "        [0.4471, 0.5529],\n",
      "        [0.4475, 0.5525],\n",
      "        [0.4454, 0.5546],\n",
      "        [0.4483, 0.5517],\n",
      "        [0.4472, 0.5528],\n",
      "        [0.4485, 0.5515],\n",
      "        [0.4480, 0.5520],\n",
      "        [0.4463, 0.5537],\n",
      "        [0.4462, 0.5538],\n",
      "        [0.4479, 0.5521],\n",
      "        [0.4454, 0.5546],\n",
      "        [0.4481, 0.5519],\n",
      "        [0.4484, 0.5516],\n",
      "        [0.4472, 0.5528],\n",
      "        [0.4461, 0.5539],\n",
      "        [0.4468, 0.5532],\n",
      "        [0.4474, 0.5526],\n",
      "        [0.4485, 0.5515],\n",
      "        [0.4452, 0.5548],\n",
      "        [0.4473, 0.5527],\n",
      "        [0.4472, 0.5528],\n",
      "        [0.4474, 0.5526],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.4463, 0.5537],\n",
      "        [0.4478, 0.5522],\n",
      "        [0.4476, 0.5524],\n",
      "        [0.4465, 0.5535],\n",
      "        [0.4477, 0.5523],\n",
      "        [0.4497, 0.5503],\n",
      "        [0.4487, 0.5513],\n",
      "        [0.4486, 0.5514],\n",
      "        [0.4463, 0.5537],\n",
      "        [0.4475, 0.5525],\n",
      "        [0.4489, 0.5511],\n",
      "        [0.4465, 0.5535],\n",
      "        [0.4457, 0.5543],\n",
      "        [0.4476, 0.5524],\n",
      "        [0.4476, 0.5524]], grad_fn=<SoftmaxBackward0>) tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "Epoch: 0. Loss: 0.2454201877117157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████▏                   | 101/225 [00:18<00:20,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0980, 0.0980, 0.0667,  ..., 0.2510, 0.1843, 0.1922],\n",
      "          [0.1373, 0.1333, 0.0941,  ..., 0.2667, 0.1647, 0.1608],\n",
      "          [0.1529, 0.1216, 0.1333,  ..., 0.1961, 0.2000, 0.1882],\n",
      "          ...,\n",
      "          [0.2980, 0.2902, 0.1961,  ..., 0.2588, 0.2196, 0.3098],\n",
      "          [0.0863, 0.1098, 0.1255,  ..., 0.2706, 0.2392, 0.2078],\n",
      "          [0.2471, 0.2510, 0.1412,  ..., 0.3098, 0.3686, 0.2784]]],\n",
      "\n",
      "\n",
      "        [[[0.5961, 0.8000, 0.6941,  ..., 0.7176, 0.9176, 0.9137],\n",
      "          [0.5529, 0.8784, 0.7412,  ..., 0.6627, 0.9216, 0.9176],\n",
      "          [0.5216, 0.9216, 0.7451,  ..., 0.9176, 0.9137, 0.9176],\n",
      "          ...,\n",
      "          [0.7569, 0.8078, 0.8275,  ..., 0.7725, 0.8588, 0.8588],\n",
      "          [0.8471, 0.8431, 0.8235,  ..., 0.8196, 0.7961, 0.7725],\n",
      "          [0.8353, 0.8235, 0.8471,  ..., 0.8706, 0.8824, 0.8353]]],\n",
      "\n",
      "\n",
      "        [[[0.4431, 0.4510, 0.4627,  ..., 0.5882, 0.5137, 0.4941],\n",
      "          [0.3882, 0.4588, 0.4745,  ..., 0.6196, 0.5255, 0.5098],\n",
      "          [0.1608, 0.4667, 0.4824,  ..., 0.6314, 0.5373, 0.5216],\n",
      "          ...,\n",
      "          [0.3961, 0.4235, 0.4275,  ..., 0.4902, 0.4941, 0.4549],\n",
      "          [0.3882, 0.4039, 0.4275,  ..., 0.4824, 0.4863, 0.3804],\n",
      "          [0.3765, 0.4000, 0.4118,  ..., 0.4706, 0.4824, 0.4745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4627, 0.4588, 0.4706,  ..., 0.7176, 0.7216, 0.7098],\n",
      "          [0.4588, 0.4667, 0.4627,  ..., 0.7098, 0.7098, 0.7098],\n",
      "          [0.4549, 0.4588, 0.4627,  ..., 0.7020, 0.7020, 0.6941],\n",
      "          ...,\n",
      "          [0.5490, 0.5490, 0.4941,  ..., 0.5020, 0.5412, 0.4902],\n",
      "          [0.5647, 0.5804, 0.5882,  ..., 0.5255, 0.4824, 0.5176],\n",
      "          [0.5412, 0.5725, 0.5961,  ..., 0.5412, 0.5059, 0.4863]]],\n",
      "\n",
      "\n",
      "        [[[0.7059, 0.7098, 0.7137,  ..., 0.4784, 0.4863, 0.4824],\n",
      "          [0.6941, 0.7059, 0.7137,  ..., 0.4824, 0.4745, 0.4745],\n",
      "          [0.7020, 0.7176, 0.7098,  ..., 0.4863, 0.4902, 0.4863],\n",
      "          ...,\n",
      "          [0.7255, 0.7255, 0.7294,  ..., 0.0588, 0.0627, 0.0627],\n",
      "          [0.7176, 0.7255, 0.7176,  ..., 0.0392, 0.0157, 0.0275],\n",
      "          [0.7216, 0.7255, 0.7216,  ..., 0.0196, 0.0353, 0.0118]]],\n",
      "\n",
      "\n",
      "        [[[0.1098, 0.0863, 0.0863,  ..., 0.3961, 0.4196, 0.4196],\n",
      "          [0.1098, 0.0863, 0.0627,  ..., 0.4314, 0.4314, 0.4039],\n",
      "          [0.1020, 0.1020, 0.0510,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          ...,\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.7216, 0.7216, 0.7216],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.7490, 0.7490, 0.7490],\n",
      "          [0.8196, 0.8196, 0.7961,  ..., 0.7490, 0.7490, 0.7490]]]])\n",
      "tensor([[0.4980, 0.5020],\n",
      "        [0.4932, 0.5068],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5039, 0.4961],\n",
      "        [0.4984, 0.5016],\n",
      "        [0.5132, 0.4868],\n",
      "        [0.5038, 0.4962],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.4985, 0.5015],\n",
      "        [0.4924, 0.5076],\n",
      "        [0.4932, 0.5068],\n",
      "        [0.5182, 0.4818],\n",
      "        [0.5069, 0.4931],\n",
      "        [0.5067, 0.4933],\n",
      "        [0.4975, 0.5025],\n",
      "        [0.4799, 0.5201],\n",
      "        [0.5014, 0.4986],\n",
      "        [0.4987, 0.5013],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5034, 0.4966],\n",
      "        [0.5044, 0.4956],\n",
      "        [0.5011, 0.4989],\n",
      "        [0.5304, 0.4696],\n",
      "        [0.4938, 0.5062],\n",
      "        [0.5099, 0.4901],\n",
      "        [0.4944, 0.5056],\n",
      "        [0.4991, 0.5009],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5083, 0.4917],\n",
      "        [0.4962, 0.5038],\n",
      "        [0.5074, 0.4926],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.4981, 0.5019],\n",
      "        [0.5100, 0.4900],\n",
      "        [0.4852, 0.5148],\n",
      "        [0.5050, 0.4950],\n",
      "        [0.5064, 0.4936],\n",
      "        [0.4807, 0.5193],\n",
      "        [0.4969, 0.5031],\n",
      "        [0.5232, 0.4768],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5053, 0.4947],\n",
      "        [0.4999, 0.5001],\n",
      "        [0.4950, 0.5050],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.4985, 0.5015],\n",
      "        [0.4874, 0.5126],\n",
      "        [0.4932, 0.5068],\n",
      "        [0.5018, 0.4982],\n",
      "        [0.5388, 0.4612],\n",
      "        [0.5204, 0.4796],\n",
      "        [0.4978, 0.5022],\n",
      "        [0.5007, 0.4993],\n",
      "        [0.4974, 0.5026],\n",
      "        [0.5023, 0.4977],\n",
      "        [0.4949, 0.5051],\n",
      "        [0.4900, 0.5100],\n",
      "        [0.4985, 0.5015],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5035, 0.4965],\n",
      "        [0.5096, 0.4904],\n",
      "        [0.5065, 0.4935],\n",
      "        [0.5073, 0.4927],\n",
      "        [0.5011, 0.4989],\n",
      "        [0.5022, 0.4978],\n",
      "        [0.4909, 0.5091],\n",
      "        [0.5038, 0.4962],\n",
      "        [0.5183, 0.4817],\n",
      "        [0.4907, 0.5093],\n",
      "        [0.4915, 0.5085],\n",
      "        [0.4815, 0.5185],\n",
      "        [0.4948, 0.5052],\n",
      "        [0.4864, 0.5136],\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4797, 0.5203],\n",
      "        [0.5013, 0.4987],\n",
      "        [0.4985, 0.5015],\n",
      "        [0.4982, 0.5018],\n",
      "        [0.4941, 0.5059],\n",
      "        [0.5034, 0.4966],\n",
      "        [0.4926, 0.5074],\n",
      "        [0.5170, 0.4830],\n",
      "        [0.4927, 0.5073],\n",
      "        [0.4990, 0.5010],\n",
      "        [0.5211, 0.4789],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5046, 0.4954],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.4952, 0.5048],\n",
      "        [0.4781, 0.5219],\n",
      "        [0.4875, 0.5125],\n",
      "        [0.4887, 0.5113],\n",
      "        [0.4924, 0.5076],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.4980, 0.5020],\n",
      "        [0.5058, 0.4942],\n",
      "        [0.4944, 0.5056],\n",
      "        [0.5021, 0.4979]], grad_fn=<SoftmaxBackward0>) tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "Epoch: 0. Loss: 0.2463034987449646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████▏   | 201/225 [00:34<00:03,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2000, 0.2627, 0.3804,  ..., 0.4510, 0.4471, 0.4549],\n",
      "          [0.2078, 0.2588, 0.3804,  ..., 0.4510, 0.4510, 0.4588],\n",
      "          [0.2118, 0.2627, 0.3765,  ..., 0.4549, 0.4549, 0.4549],\n",
      "          ...,\n",
      "          [0.4314, 0.4706, 0.4980,  ..., 0.3373, 0.3490, 0.3333],\n",
      "          [0.4824, 0.4902, 0.4549,  ..., 0.3255, 0.3451, 0.3294],\n",
      "          [0.4196, 0.4588, 0.4392,  ..., 0.3412, 0.3373, 0.3333]]],\n",
      "\n",
      "\n",
      "        [[[0.4275, 0.4392, 0.4941,  ..., 0.0275, 0.0431, 0.0353],\n",
      "          [0.4667, 0.4745, 0.4078,  ..., 0.0667, 0.0745, 0.0784],\n",
      "          [0.4314, 0.3608, 0.3686,  ..., 0.1686, 0.2392, 0.1490],\n",
      "          ...,\n",
      "          [0.2863, 0.3765, 0.4275,  ..., 0.0863, 0.0706, 0.1294],\n",
      "          [0.2824, 0.3333, 0.4235,  ..., 0.0784, 0.0667, 0.1059],\n",
      "          [0.2471, 0.3020, 0.3176,  ..., 0.0667, 0.0549, 0.0824]]],\n",
      "\n",
      "\n",
      "        [[[0.9333, 0.9216, 0.7647,  ..., 0.4118, 0.2392, 0.5137],\n",
      "          [0.9020, 0.8980, 0.7176,  ..., 0.3608, 0.2980, 0.4078],\n",
      "          [0.9216, 0.8863, 0.9176,  ..., 0.1333, 0.1333, 0.1569],\n",
      "          ...,\n",
      "          [0.3529, 0.4078, 0.4157,  ..., 0.9255, 0.9412, 0.8902],\n",
      "          [0.4118, 0.4824, 0.4941,  ..., 0.9216, 0.9216, 0.9373],\n",
      "          [0.0941, 0.5020, 0.5098,  ..., 0.9098, 0.9098, 0.9255]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.5137, 0.3451, 0.2980,  ..., 0.8980, 0.9176, 0.8431],\n",
      "          [0.5843, 0.2431, 0.2667,  ..., 0.8745, 0.8392, 0.7098],\n",
      "          [0.5765, 0.2588, 0.2196,  ..., 0.8824, 0.8000, 0.6471],\n",
      "          ...,\n",
      "          [0.3843, 0.4863, 0.5412,  ..., 0.4118, 0.3451, 0.4039],\n",
      "          [0.6627, 0.5451, 0.4353,  ..., 0.3647, 0.4235, 0.2745],\n",
      "          [0.3098, 0.3333, 0.3176,  ..., 0.4549, 0.1294, 0.3882]]],\n",
      "\n",
      "\n",
      "        [[[0.7490, 0.8235, 0.8196,  ..., 0.3137, 0.2784, 0.0980],\n",
      "          [0.8078, 0.7961, 0.8196,  ..., 0.3176, 0.2863, 0.0824],\n",
      "          [0.6980, 0.7765, 0.8196,  ..., 0.3373, 0.2863, 0.0706],\n",
      "          ...,\n",
      "          [0.8353, 0.9020, 0.7020,  ..., 0.9961, 0.9882, 0.9922],\n",
      "          [0.8588, 0.9020, 0.7804,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9922, 0.9529, 0.9020,  ..., 0.9961, 0.9961, 0.9961]]],\n",
      "\n",
      "\n",
      "        [[[0.2471, 0.2431, 0.2431,  ..., 0.1569, 0.1294, 0.1569],\n",
      "          [0.2431, 0.2510, 0.2392,  ..., 0.1490, 0.1529, 0.1529],\n",
      "          [0.2392, 0.2235, 0.2196,  ..., 0.1490, 0.1490, 0.1569],\n",
      "          ...,\n",
      "          [0.5804, 0.5804, 0.5725,  ..., 0.1765, 0.1804, 0.1961],\n",
      "          [0.5490, 0.5529, 0.5843,  ..., 0.1765, 0.1843, 0.1843],\n",
      "          [0.5647, 0.5725, 0.5490,  ..., 0.2118, 0.1922, 0.1843]]]])\n",
      "tensor([[0.4920, 0.5080],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.4632, 0.5368],\n",
      "        [0.5031, 0.4969],\n",
      "        [0.5374, 0.4626],\n",
      "        [0.5251, 0.4749],\n",
      "        [0.5084, 0.4916],\n",
      "        [0.4768, 0.5232],\n",
      "        [0.4544, 0.5456],\n",
      "        [0.5203, 0.4797],\n",
      "        [0.5169, 0.4831],\n",
      "        [0.5366, 0.4634],\n",
      "        [0.5105, 0.4895],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.4557, 0.5443],\n",
      "        [0.5094, 0.4906],\n",
      "        [0.5688, 0.4312],\n",
      "        [0.4746, 0.5254],\n",
      "        [0.4573, 0.5427],\n",
      "        [0.4395, 0.5605],\n",
      "        [0.4276, 0.5724],\n",
      "        [0.5459, 0.4541],\n",
      "        [0.5058, 0.4942],\n",
      "        [0.4886, 0.5114],\n",
      "        [0.4281, 0.5719],\n",
      "        [0.4814, 0.5186],\n",
      "        [0.4875, 0.5125],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.4605, 0.5395],\n",
      "        [0.4379, 0.5621],\n",
      "        [0.5922, 0.4078],\n",
      "        [0.4809, 0.5191],\n",
      "        [0.5908, 0.4092],\n",
      "        [0.4959, 0.5041],\n",
      "        [0.4739, 0.5261],\n",
      "        [0.4311, 0.5689],\n",
      "        [0.4837, 0.5163],\n",
      "        [0.4723, 0.5277],\n",
      "        [0.4680, 0.5320],\n",
      "        [0.4758, 0.5242],\n",
      "        [0.4555, 0.5445],\n",
      "        [0.5052, 0.4948],\n",
      "        [0.5891, 0.4109],\n",
      "        [0.4918, 0.5082],\n",
      "        [0.4687, 0.5313],\n",
      "        [0.4357, 0.5643],\n",
      "        [0.5181, 0.4819],\n",
      "        [0.4340, 0.5660],\n",
      "        [0.5587, 0.4413],\n",
      "        [0.6128, 0.3872],\n",
      "        [0.5013, 0.4987],\n",
      "        [0.4561, 0.5439],\n",
      "        [0.5212, 0.4788],\n",
      "        [0.4811, 0.5189],\n",
      "        [0.5066, 0.4934],\n",
      "        [0.5080, 0.4920],\n",
      "        [0.4372, 0.5628],\n",
      "        [0.4954, 0.5046],\n",
      "        [0.4698, 0.5302],\n",
      "        [0.5700, 0.4300],\n",
      "        [0.4289, 0.5711],\n",
      "        [0.5824, 0.4176],\n",
      "        [0.4330, 0.5670],\n",
      "        [0.4513, 0.5487],\n",
      "        [0.4769, 0.5231],\n",
      "        [0.4433, 0.5567],\n",
      "        [0.4704, 0.5296],\n",
      "        [0.4472, 0.5528],\n",
      "        [0.5506, 0.4494],\n",
      "        [0.4530, 0.5470],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.5398, 0.4602],\n",
      "        [0.5283, 0.4717],\n",
      "        [0.5090, 0.4910],\n",
      "        [0.5451, 0.4549],\n",
      "        [0.4813, 0.5187],\n",
      "        [0.5491, 0.4509],\n",
      "        [0.4445, 0.5555],\n",
      "        [0.4528, 0.5472],\n",
      "        [0.5264, 0.4736],\n",
      "        [0.4478, 0.5522],\n",
      "        [0.5065, 0.4935],\n",
      "        [0.4267, 0.5733],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.4420, 0.5580],\n",
      "        [0.4545, 0.5455],\n",
      "        [0.5324, 0.4676],\n",
      "        [0.4874, 0.5126],\n",
      "        [0.4442, 0.5558],\n",
      "        [0.4747, 0.5253],\n",
      "        [0.4786, 0.5214],\n",
      "        [0.5350, 0.4650],\n",
      "        [0.5027, 0.4973],\n",
      "        [0.4866, 0.5134],\n",
      "        [0.4410, 0.5590],\n",
      "        [0.5591, 0.4409],\n",
      "        [0.5399, 0.4601],\n",
      "        [0.5075, 0.4925],\n",
      "        [0.4260, 0.5740],\n",
      "        [0.5156, 0.4844]], grad_fn=<SoftmaxBackward0>) tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "Epoch: 0. Loss: 0.24749858677387238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:38<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        net.zero_grad()\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        if i%10000==0:\n",
    "            print(batch_X)\n",
    "            print(outputs,batch_y)\n",
    "            print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055558b6-d709-411e-bd66-cc9b1a413723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 2494/2494 [00:04<00:00, 611.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f628124-e398-4e27-9c72-0bc6dbbf924a",
   "metadata": {},
   "source": [
    "## Training On GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e133859a-47ed-4816-af99-bbbc365a64ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "baac1bc4-80f8-4aaf-856d-18d3854163a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "564107ee-3d8b-4ab0-8f4d-e0d5c282da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5260e47-81bc-4a17-afa3-dd341307b7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a44c4411-c83b-43a9-b209-69970399c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 81.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.2358788102865219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 88.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.19341233372688293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 88.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.14679287374019623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.12288574874401093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.10470270365476608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 88.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Loss: 0.09223631769418716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6. Loss: 0.08314571529626846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7. Loss: 0.07610497623682022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Loss: 0.07063352316617966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9. Loss: 0.06532035768032074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Loss: 0.05949827656149864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Loss: 0.0542074553668499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12. Loss: 0.05047612637281418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13. Loss: 0.04632323235273361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14. Loss: 0.042012590914964676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15. Loss: 0.037352632731199265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 88.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16. Loss: 0.03495047613978386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17. Loss: 0.0328926257789135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18. Loss: 0.022412855178117752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19. Loss: 0.017423516139388084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20. Loss: 0.01435967069119215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21. Loss: 0.014455851167440414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22. Loss: 0.02061457745730877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23. Loss: 0.0248182974755764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24. Loss: 0.006939131300896406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25. Loss: 0.007720933761447668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26. Loss: 0.007109211292117834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27. Loss: 0.007647277321666479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28. Loss: 0.012831190600991249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29. Loss: 0.02658802829682827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30. Loss: 0.02672279067337513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31. Loss: 0.019203132018446922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32. Loss: 0.03838939219713211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33. Loss: 0.0170846376568079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34. Loss: 0.009216934442520142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35. Loss: 0.0032603817526251078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36. Loss: 0.002317760605365038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37. Loss: 0.0031479408498853445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38. Loss: 0.0024934441316872835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 225/225 [00:02<00:00, 87.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39. Loss: 0.002891820389777422\n",
      "CPU times: total: 8.89 s\n",
      "Wall time: 1min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optim.Adam(net.parameters(),0.0003)\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "Epochs = 40\n",
    "BATCH_SIZE = 100\n",
    "def train(net):\n",
    "    for epoch in range(Epochs):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50).to(device)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE].to(device)\n",
    "            net.zero_grad()\n",
    "            \n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f01edd92-b51e-411d-9c71-9ff797d2199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 2494/2494 [00:03<00:00, 745.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  # returns a list, \n",
    "            predicted_class = torch.argmax(net_out)\n",
    "    \n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy: \", round(correct/total, 3))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96803a2-7b8d-4e8f-a0fc-012af51138d4",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8addf-b85b-4320-ab9c-718b8eb215d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
