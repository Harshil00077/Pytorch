{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01786b3-7f37-4b62-a04d-1ec0a308aea9",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcd4d1a-b1fd-4a9a-a083-895d7a22bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "Rebuild_Data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284794a9-0ce8-40b8-8b9a-23b2bd5dea2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e9f1c0-6c18-4462-b949-15253047af13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12501/12501 [00:13<00:00, 936.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12501/12501 [00:13<00:00, 905.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 24946/24946 [00:00<00:00, 1857334.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DogsVSCats:\n",
    "    IMG_SIZE = (50, 50)\n",
    "    CATS = \"../PetImages/Cat\"\n",
    "    DOGS = \"../PetImages/Dog\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_data = []\n",
    "        self.catcount = 0\n",
    "        self.dogcount = 0\n",
    "        self.hg_data = []\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                path = os.path.join(label, f)\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                one_hot_label = np.eye(2)[self.LABELS[label]]\n",
    "               \n",
    "\n",
    "                if img is not None:\n",
    "                    try:\n",
    "                        img = cv2.resize(img, self.IMG_SIZE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to process {f}: {e}\")\n",
    "                        continue  # Skip this image if an error occurs\n",
    "                    self.training_data.append(np.array([np.array(img), one_hot_label],dtype=\"object\"))\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount += 1\n",
    "\n",
    "        for data in tqdm(self.training_data):\n",
    "            if (len(data) == 2 and isinstance(data[0], np.ndarray) and data[0].shape == self.IMG_SIZE and isinstance(data[1], np.ndarray) and data[1].shape == (2,)):\n",
    "                self.hg_data.append(data)\n",
    "            else:\n",
    "                print(f\"Skipping inhomogeneous element: {data}\")\n",
    "\n",
    "        # Save the homogeneous data with allow_pickle=True\n",
    "        np.save(\"training_data.npy\",self.hg_data, allow_pickle=True)\n",
    "        print(\"Cats:\", self.catcount)\n",
    "        print(\"Dogs:\", self.dogcount)\n",
    "\n",
    "# Example usage\n",
    "Rebuild_Data = True\n",
    "if Rebuild_Data:\n",
    "    dataset = DogsVSCats()\n",
    "    dataset.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e86f68d-35d1-44c1-be2e-354b661b4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a1c759-92f5-4bc7-bae1-7827aec7fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5d7f35-5f6e-4853-a31b-65fd48a0d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 44,  60,  41, ...,  95,  94, 197],\n",
      "        [ 41,  43,  43, ...,  93,  80, 192],\n",
      "        [ 41,  40,  46, ...,  90,  86, 193],\n",
      "        ...,\n",
      "        [ 25,  21,  26, ...,  43,  67,  73],\n",
      "        [ 21,  23,  21, ...,  74,  36,  89],\n",
      "        [ 23,  22,  20, ...,  59,  62,  32]], dtype=uint8) array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b60446-a640-45a3-a404-97bc8f7c8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(training_data[11][0],cmap=\"gray\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd56a-eaa4-4fdf-a612-6f89894313e2",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97facfe7-1655-4eb7-9fdd-2a9a5602a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 50, 50])\n",
      "After conv1: torch.Size([1, 32, 46, 46])\n",
      "After pool1: torch.Size([1, 32, 23, 23])\n",
      "After conv2: torch.Size([1, 64, 19, 19])\n",
      "After pool2: torch.Size([1, 64, 9, 9])\n",
      "After conv3: torch.Size([1, 128, 5, 5])\n",
      "After pool3: torch.Size([1, 128, 2, 2])\n",
      "After flatten: torch.Size([1, 512])\n",
      "After fc1: torch.Size([1, 16])\n",
      "After fc2: torch.Size([1, 2])\n",
      "Output: tensor([[-0.1590, -0.2967]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 1)  # 5x5 kernel, 1 input channel, 32 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1) # 5x5 kernel, 32 input channels, 64 output channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 1) # 5x5 kernel, 64 input channels, 128 output channels\n",
    "        # Calculate the flattened size after conv and pooling layers.\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 16)  # Update the input size accordingly\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(\"After conv1:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool1:\", x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(\"After conv2:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool2:\", x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(\"After conv3:\", x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        print(\"After pool3:\", x.shape)\n",
    "        x = x.view(-1, 128 * 2 * 2)  # Flatten the tensor\n",
    "        print(\"After flatten:\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"After fc1:\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"After fc2:\", x.shape)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "net = Net()\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 1, 50, 50)  # Batch size of 1, 1 channel, 50x50 image\n",
    "\n",
    "output = net.forward(dummy_input)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447683ad-da37-40d9-8158-6062b1e4da24",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "# Hidden Layers\n",
    "\n",
    "**Role:**\n",
    "- The hidden layers transform the input data into a space where the patterns relevant to the problem can be more easily separated.\n",
    "- These layers create complex, non-linear mappings of the input data.\n",
    "\n",
    "## **Common Activation Functions:**\n",
    "## $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "  - Pros: Helps with the vanishing gradient problem, computationally efficient.\n",
    "  - Cons: Can suffer from the \"dying ReLU\" problem where neurons can get stuck during training.\n",
    "## $ \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} $\n",
    "  - Pros: Outputs values between 0 and 1, can be interpreted as probabilities.\n",
    "  - Cons: Can cause vanishing gradient problems.\n",
    "## $ \\text{Tanh}(x) =  \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "  - Pros: Outputs values between -1 and 1, zero-centered.\n",
    "  - Cons: Also suffers from vanishing gradient issues.\n",
    "\n",
    "# Output Layers\n",
    "\n",
    "**Role:**\n",
    "- The output layer produces the final result of the neural network, which could be a classification, a regression value, etc.\n",
    "- Different problems require different output ranges and interpretations.\n",
    "\n",
    "# **Common Activation Functions for Output Layers:**\n",
    "## **Softmax:** Typically used in multi-class classification problems.\n",
    "> ## $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $\n",
    "  - Pros: Converts logits into probabilities that sum to 1.\n",
    "  - Cons: Not suitable for regression tasks.\n",
    "## **Sigmoid:** Often used in binary classification problems.\n",
    "> ## $ \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} $\n",
    "  - Pros: Outputs values between 0 and 1.\n",
    "  - Cons: Limited to binary outcomes.\n",
    "## **Linear:** Used in regression problems.\n",
    "> ## $ \\text{Linear}(x) = x $\n",
    "  - Pros: Suitable for predicting continuous values.\n",
    "  - Cons: Not suitable for classification tasks.\n",
    "\n",
    "# Why Not Use ReLU in the Output Layer?\n",
    "\n",
    "**ReLU Characteristics:**\n",
    "- Outputs any positive value as itself and zero for any negative input.\n",
    "- Not bounded, meaning its output range is $[0, \\infty)$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "1. **Classification Problem:**\n",
    "   - If you use ReLU in the output layer for a classification problem (e.g., a multi-class classification task), you would get non-negative outputs but not probabilities. Classification typically requires interpreting the output as probabilities, which should sum to 1. ReLU cannot naturally enforce this constraint, unlike the Softmax function.\n",
    "\n",
    "2. **Regression Problem:**\n",
    "   - For regression tasks, using ReLU could limit the range of the output to non-negative values only. This might not be desirable if the target values can be negative. For example, predicting house prices could work with ReLU if prices are always positive, but predicting temperature, which can be negative, would be inappropriate with ReLU.\n",
    "\n",
    "### Detailed Example: Multi-Class Classification\n",
    "\n",
    "Consider a neural network designed to classify images into one of three categories: Cat, Dog, and Bird. The final layer must output a probability distribution over these three classes.\n",
    "\n",
    "**Using ReLU in the Output Layer:**\n",
    "- The network might output values like [2.5, 0, 3.7] for an input image.\n",
    "- These values are not probabilities and do not sum to 1.\n",
    "- There's no straightforward way to interpret these values as the likelihood of each class.\n",
    "\n",
    "**Using Softmax in the Output Layer:**\n",
    "- The network might output logits [2.5, 0, 3.7].\n",
    "- After applying Softmax, these logits might convert to probabilities like [0.28, 0.01, 0.71].\n",
    "- These probabilities sum to 1, making it clear that the model predicts \"Bird\" with 71% confidence.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The choice of activation function in the output layer is crucial and must align with the problem requirements. While ReLU is excellent for hidden layers due to its properties that help mitigate issues like the vanishing gradient problem, it is not suitable for output layers in many cases because it does not provide a bounded, interpretable output suitable for classification or regression tasks. Different tasks necessitate different activation functions to ensure the outputs are meaningful and useful for the specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56831888-46f8-4351-8d8e-aa5948188a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4883, 0.5117]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, 5, 1)  # 5x5 kernel, 1 input channel, 32 output channels\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 5, 1) # 5x5 kernel, 32 input channels, 64 output channels\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 5, 1) # 5x5 kernel, 64 input channels, 128 output channels\n",
    "#         # Calculate the flattened size after conv and pooling layers.\n",
    "#         self.fc1 = nn.Linear(128 * 2 * 2, 16)  # Update the input size accordingly\n",
    "#         self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = x.view(-1, 128 * 2 * 2)  \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return F.softmax(x,dim=1)\n",
    "\n",
    "# # Example usage\n",
    "# net = Net()\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 1, 50, 50)  # Batch size of 1, 1 channel, 50x50 image\n",
    "\n",
    "output = net.forward(dummy_input)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82cda75-482a-4a7f-90f7-cd9dfc017bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),0.0001)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f46b446-ba88-400f-807f-c946b879440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opdar\\AppData\\Local\\Temp\\ipykernel_13700\\3353633519.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ad9152-464c-47ad-9146-53cef3eaabc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae847a5-6298-4488-afa9-0d56b3ec1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "# train_X and train_y will contain data for training\n",
    "# test_X and test_y will contain data for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77ad59-8fc2-4da8-b6c7-51927cbebbe8",
   "metadata": {},
   "source": [
    "The main difference between `model.zero_grads()` and `optimizer.zero_grads()` lies in the scope of the operation:\r\n",
    "\r\n",
    "- `model.zero_grads()`: This method zeroes the gradients of all parameters in the model. It directly operates on the model itself, making it suitable for cases where you want to manually manage the gradient clearing process within the model.\r\n",
    "\r\n",
    "- `optimizer.zero_grads()`: On the other hand, this method zeroes the gradients of only the parameters that the optimizer is responsible for updating. It operates on the optimizer object and is commonly used during the training loop after the backward pass to clear gradients before the optimizer updates the model parametmized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f973efb3-d5e5-4d71-b989-167da8f9f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 2/225 [00:00<00:25,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6275, 0.5882, 0.6275,  ..., 0.4078, 0.4118, 0.3765],\n",
      "          [0.6784, 0.6118, 0.6078,  ..., 0.3765, 0.3020, 0.2824],\n",
      "          [0.6824, 0.6588, 0.6196,  ..., 0.2471, 0.2980, 0.3647],\n",
      "          ...,\n",
      "          [0.9647, 0.9451, 0.9451,  ..., 0.7725, 0.7686, 0.7255],\n",
      "          [0.9529, 0.9373, 0.9373,  ..., 0.8235, 0.7686, 0.7294],\n",
      "          [0.9765, 0.9765, 0.9686,  ..., 0.8431, 0.9176, 0.7765]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 0.9922,  ..., 0.5255, 0.5529, 0.4980],\n",
      "          [0.9961, 0.9922, 0.9882,  ..., 0.3765, 0.5765, 0.5529],\n",
      "          [0.9922, 0.9961, 0.9961,  ..., 0.6314, 0.6196, 0.5255],\n",
      "          ...,\n",
      "          [0.4000, 0.5765, 0.5569,  ..., 0.7804, 0.7882, 0.7961],\n",
      "          [0.4510, 0.5765, 0.4902,  ..., 0.7412, 0.7333, 0.7569],\n",
      "          [0.4627, 0.5412, 0.4000,  ..., 0.7529, 0.7176, 0.4627]]],\n",
      "\n",
      "\n",
      "        [[[0.0275, 0.0353, 0.0314,  ..., 0.5922, 0.3804, 0.2000],\n",
      "          [0.0314, 0.0314, 0.0392,  ..., 0.6118, 0.4784, 0.2275],\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.6118, 0.5020, 0.2980],\n",
      "          ...,\n",
      "          [0.5176, 0.4941, 0.4275,  ..., 0.2745, 0.1961, 0.1961],\n",
      "          [0.5608, 0.5569, 0.5961,  ..., 0.2235, 0.2431, 0.2784],\n",
      "          [0.5451, 0.5373, 0.5412,  ..., 0.2000, 0.1765, 0.1529]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2667, 0.3098, 0.3216,  ..., 0.3608, 0.3294, 0.0863],\n",
      "          [0.2706, 0.3216, 0.3373,  ..., 0.3882, 0.3451, 0.0941],\n",
      "          [0.2902, 0.3255, 0.3490,  ..., 0.4039, 0.3686, 0.1137],\n",
      "          ...,\n",
      "          [0.3098, 0.3412, 0.3333,  ..., 0.4118, 0.3843, 0.1333],\n",
      "          [0.2863, 0.3216, 0.2980,  ..., 0.3882, 0.3569, 0.1255],\n",
      "          [0.2627, 0.2902, 0.2549,  ..., 0.3647, 0.3216, 0.1216]]],\n",
      "\n",
      "\n",
      "        [[[0.4314, 0.4510, 0.4784,  ..., 0.3294, 0.4314, 0.4000],\n",
      "          [0.4549, 0.4863, 0.4902,  ..., 0.4275, 0.4275, 0.4078],\n",
      "          [0.4745, 0.5176, 0.3647,  ..., 0.3569, 0.4196, 0.4353],\n",
      "          ...,\n",
      "          [0.4510, 0.4784, 0.5098,  ..., 0.5137, 0.4863, 0.4706],\n",
      "          [0.4510, 0.4745, 0.4980,  ..., 0.4941, 0.4902, 0.4706],\n",
      "          [0.4157, 0.4353, 0.4745,  ..., 0.5020, 0.4902, 0.4471]]],\n",
      "\n",
      "\n",
      "        [[[0.2863, 0.2824, 0.2902,  ..., 0.7569, 0.7922, 0.8392],\n",
      "          [0.2667, 0.2627, 0.2667,  ..., 0.6902, 0.7137, 0.7569],\n",
      "          [0.2510, 0.2431, 0.2431,  ..., 0.6275, 0.6314, 0.7020],\n",
      "          ...,\n",
      "          [0.0784, 0.0863, 0.0980,  ..., 0.4627, 0.5882, 0.5804],\n",
      "          [0.0706, 0.0784, 0.0745,  ..., 0.5098, 0.5961, 0.5608],\n",
      "          [0.0667, 0.0784, 0.0706,  ..., 0.5098, 0.6118, 0.3961]]]])\n",
      "tensor([[3.2075e-03, 9.9679e-01],\n",
      "        [2.2950e-04, 9.9977e-01],\n",
      "        [2.4299e-04, 9.9976e-01],\n",
      "        [1.0459e-03, 9.9895e-01],\n",
      "        [2.1077e-03, 9.9789e-01],\n",
      "        [3.2339e-04, 9.9968e-01],\n",
      "        [4.4861e-04, 9.9955e-01],\n",
      "        [2.3341e-03, 9.9767e-01],\n",
      "        [5.0636e-05, 9.9995e-01],\n",
      "        [5.3439e-05, 9.9995e-01],\n",
      "        [8.3686e-05, 9.9992e-01],\n",
      "        [1.0148e-04, 9.9990e-01],\n",
      "        [1.3944e-04, 9.9986e-01],\n",
      "        [6.9534e-04, 9.9930e-01],\n",
      "        [4.7073e-04, 9.9953e-01],\n",
      "        [8.7746e-04, 9.9912e-01],\n",
      "        [5.9373e-03, 9.9406e-01],\n",
      "        [2.4298e-04, 9.9976e-01],\n",
      "        [1.6207e-04, 9.9984e-01],\n",
      "        [1.1170e-04, 9.9989e-01],\n",
      "        [1.9090e-05, 9.9998e-01],\n",
      "        [2.9562e-04, 9.9970e-01],\n",
      "        [4.1657e-03, 9.9583e-01],\n",
      "        [4.7653e-05, 9.9995e-01],\n",
      "        [4.2145e-04, 9.9958e-01],\n",
      "        [1.8199e-05, 9.9998e-01],\n",
      "        [3.8164e-04, 9.9962e-01],\n",
      "        [1.9903e-04, 9.9980e-01],\n",
      "        [2.4196e-04, 9.9976e-01],\n",
      "        [3.0702e-04, 9.9969e-01],\n",
      "        [1.5114e-04, 9.9985e-01],\n",
      "        [1.7637e-04, 9.9982e-01],\n",
      "        [1.2916e-03, 9.9871e-01],\n",
      "        [1.2165e-04, 9.9988e-01],\n",
      "        [4.2557e-04, 9.9957e-01],\n",
      "        [1.3905e-04, 9.9986e-01],\n",
      "        [2.5851e-05, 9.9997e-01],\n",
      "        [1.2547e-04, 9.9987e-01],\n",
      "        [2.7171e-03, 9.9728e-01],\n",
      "        [6.5427e-04, 9.9935e-01],\n",
      "        [3.6439e-04, 9.9964e-01],\n",
      "        [2.6257e-05, 9.9997e-01],\n",
      "        [1.2497e-03, 9.9875e-01],\n",
      "        [3.0624e-04, 9.9969e-01],\n",
      "        [7.3408e-05, 9.9993e-01],\n",
      "        [5.8491e-04, 9.9942e-01],\n",
      "        [3.8746e-04, 9.9961e-01],\n",
      "        [1.4699e-04, 9.9985e-01],\n",
      "        [2.9832e-04, 9.9970e-01],\n",
      "        [2.6362e-03, 9.9736e-01],\n",
      "        [2.8103e-04, 9.9972e-01],\n",
      "        [4.8464e-05, 9.9995e-01],\n",
      "        [1.8569e-03, 9.9814e-01],\n",
      "        [4.2319e-05, 9.9996e-01],\n",
      "        [5.1993e-05, 9.9995e-01],\n",
      "        [3.7353e-05, 9.9996e-01],\n",
      "        [6.5005e-04, 9.9935e-01],\n",
      "        [1.5243e-03, 9.9848e-01],\n",
      "        [1.9702e-04, 9.9980e-01],\n",
      "        [5.4943e-03, 9.9451e-01],\n",
      "        [1.7280e-04, 9.9983e-01],\n",
      "        [1.6117e-05, 9.9998e-01],\n",
      "        [2.3904e-04, 9.9976e-01],\n",
      "        [7.9199e-04, 9.9921e-01],\n",
      "        [1.9777e-02, 9.8022e-01],\n",
      "        [7.5906e-05, 9.9992e-01],\n",
      "        [6.3635e-04, 9.9936e-01],\n",
      "        [2.9818e-04, 9.9970e-01],\n",
      "        [6.9335e-04, 9.9931e-01],\n",
      "        [1.4549e-04, 9.9985e-01],\n",
      "        [1.7523e-03, 9.9825e-01],\n",
      "        [2.8068e-04, 9.9972e-01],\n",
      "        [1.6158e-03, 9.9838e-01],\n",
      "        [2.1573e-03, 9.9784e-01],\n",
      "        [3.2119e-04, 9.9968e-01],\n",
      "        [5.8165e-05, 9.9994e-01],\n",
      "        [1.4894e-02, 9.8511e-01],\n",
      "        [1.9156e-03, 9.9808e-01],\n",
      "        [9.0726e-05, 9.9991e-01],\n",
      "        [3.3109e-05, 9.9997e-01],\n",
      "        [2.1139e-02, 9.7886e-01],\n",
      "        [1.1286e-03, 9.9887e-01],\n",
      "        [8.7009e-04, 9.9913e-01],\n",
      "        [2.6724e-04, 9.9973e-01],\n",
      "        [2.5885e-04, 9.9974e-01],\n",
      "        [3.1535e-02, 9.6846e-01],\n",
      "        [2.2680e-04, 9.9977e-01],\n",
      "        [2.4217e-04, 9.9976e-01],\n",
      "        [3.3590e-03, 9.9664e-01],\n",
      "        [1.1387e-04, 9.9989e-01],\n",
      "        [1.8017e-04, 9.9982e-01],\n",
      "        [2.0243e-04, 9.9980e-01],\n",
      "        [9.1225e-05, 9.9991e-01],\n",
      "        [8.5013e-04, 9.9915e-01],\n",
      "        [3.0887e-04, 9.9969e-01],\n",
      "        [4.7604e-05, 9.9995e-01],\n",
      "        [1.4047e-03, 9.9860e-01],\n",
      "        [2.2783e-04, 9.9977e-01],\n",
      "        [3.8896e-04, 9.9961e-01],\n",
      "        [3.2650e-04, 9.9967e-01]], grad_fn=<SoftmaxBackward0>) tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "Epoch: 0. Loss: 0.42818713188171387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▎                                           | 102/225 [00:11<00:13,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0980, 0.0980, 0.0667,  ..., 0.2510, 0.1843, 0.1922],\n",
      "          [0.1373, 0.1333, 0.0941,  ..., 0.2667, 0.1647, 0.1608],\n",
      "          [0.1529, 0.1216, 0.1333,  ..., 0.1961, 0.2000, 0.1882],\n",
      "          ...,\n",
      "          [0.2980, 0.2902, 0.1961,  ..., 0.2588, 0.2196, 0.3098],\n",
      "          [0.0863, 0.1098, 0.1255,  ..., 0.2706, 0.2392, 0.2078],\n",
      "          [0.2471, 0.2510, 0.1412,  ..., 0.3098, 0.3686, 0.2784]]],\n",
      "\n",
      "\n",
      "        [[[0.5961, 0.8000, 0.6941,  ..., 0.7176, 0.9176, 0.9137],\n",
      "          [0.5529, 0.8784, 0.7412,  ..., 0.6627, 0.9216, 0.9176],\n",
      "          [0.5216, 0.9216, 0.7451,  ..., 0.9176, 0.9137, 0.9176],\n",
      "          ...,\n",
      "          [0.7569, 0.8078, 0.8275,  ..., 0.7725, 0.8588, 0.8588],\n",
      "          [0.8471, 0.8431, 0.8235,  ..., 0.8196, 0.7961, 0.7725],\n",
      "          [0.8353, 0.8235, 0.8471,  ..., 0.8706, 0.8824, 0.8353]]],\n",
      "\n",
      "\n",
      "        [[[0.4431, 0.4510, 0.4627,  ..., 0.5882, 0.5137, 0.4941],\n",
      "          [0.3882, 0.4588, 0.4745,  ..., 0.6196, 0.5255, 0.5098],\n",
      "          [0.1608, 0.4667, 0.4824,  ..., 0.6314, 0.5373, 0.5216],\n",
      "          ...,\n",
      "          [0.3961, 0.4235, 0.4275,  ..., 0.4902, 0.4941, 0.4549],\n",
      "          [0.3882, 0.4039, 0.4275,  ..., 0.4824, 0.4863, 0.3804],\n",
      "          [0.3765, 0.4000, 0.4118,  ..., 0.4706, 0.4824, 0.4745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4627, 0.4588, 0.4706,  ..., 0.7176, 0.7216, 0.7098],\n",
      "          [0.4588, 0.4667, 0.4627,  ..., 0.7098, 0.7098, 0.7098],\n",
      "          [0.4549, 0.4588, 0.4627,  ..., 0.7020, 0.7020, 0.6941],\n",
      "          ...,\n",
      "          [0.5490, 0.5490, 0.4941,  ..., 0.5020, 0.5412, 0.4902],\n",
      "          [0.5647, 0.5804, 0.5882,  ..., 0.5255, 0.4824, 0.5176],\n",
      "          [0.5412, 0.5725, 0.5961,  ..., 0.5412, 0.5059, 0.4863]]],\n",
      "\n",
      "\n",
      "        [[[0.7059, 0.7098, 0.7137,  ..., 0.4784, 0.4863, 0.4824],\n",
      "          [0.6941, 0.7059, 0.7137,  ..., 0.4824, 0.4745, 0.4745],\n",
      "          [0.7020, 0.7176, 0.7098,  ..., 0.4863, 0.4902, 0.4863],\n",
      "          ...,\n",
      "          [0.7255, 0.7255, 0.7294,  ..., 0.0588, 0.0627, 0.0627],\n",
      "          [0.7176, 0.7255, 0.7176,  ..., 0.0392, 0.0157, 0.0275],\n",
      "          [0.7216, 0.7255, 0.7216,  ..., 0.0196, 0.0353, 0.0118]]],\n",
      "\n",
      "\n",
      "        [[[0.1098, 0.0863, 0.0863,  ..., 0.3961, 0.4196, 0.4196],\n",
      "          [0.1098, 0.0863, 0.0627,  ..., 0.4314, 0.4314, 0.4039],\n",
      "          [0.1020, 0.1020, 0.0510,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          ...,\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.7216, 0.7216, 0.7216],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.7490, 0.7490, 0.7490],\n",
      "          [0.8196, 0.8196, 0.7961,  ..., 0.7490, 0.7490, 0.7490]]]])\n",
      "tensor([[0.5114, 0.4886],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5160, 0.4840],\n",
      "        [0.5190, 0.4810],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5102, 0.4898],\n",
      "        [0.5208, 0.4792],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5076, 0.4924],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5177, 0.4823],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5180, 0.4820],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5104, 0.4896],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5121, 0.4879],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5071, 0.4929],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5101, 0.4899],\n",
      "        [0.5101, 0.4899],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5190, 0.4810],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5087, 0.4913],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5088, 0.4912],\n",
      "        [0.5169, 0.4831],\n",
      "        [0.5096, 0.4904],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5164, 0.4836],\n",
      "        [0.5190, 0.4810],\n",
      "        [0.5084, 0.4916],\n",
      "        [0.5074, 0.4926],\n",
      "        [0.5206, 0.4794],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5080, 0.4920],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5094, 0.4906],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5196, 0.4804],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5089, 0.4911],\n",
      "        [0.5050, 0.4950],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5121, 0.4879],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5132, 0.4868],\n",
      "        [0.5118, 0.4882],\n",
      "        [0.5118, 0.4882],\n",
      "        [0.5102, 0.4898],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5078, 0.4922],\n",
      "        [0.5110, 0.4890],\n",
      "        [0.5188, 0.4812],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5199, 0.4801],\n",
      "        [0.5196, 0.4804],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5084, 0.4916],\n",
      "        [0.5092, 0.4908],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5180, 0.4820],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5149, 0.4851],\n",
      "        [0.5053, 0.4947],\n",
      "        [0.5085, 0.4915]], grad_fn=<SoftmaxBackward0>) tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "Epoch: 0. Loss: 0.25150012969970703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████▊        | 202/225 [00:23<00:02,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2000, 0.2627, 0.3804,  ..., 0.4510, 0.4471, 0.4549],\n",
      "          [0.2078, 0.2588, 0.3804,  ..., 0.4510, 0.4510, 0.4588],\n",
      "          [0.2118, 0.2627, 0.3765,  ..., 0.4549, 0.4549, 0.4549],\n",
      "          ...,\n",
      "          [0.4314, 0.4706, 0.4980,  ..., 0.3373, 0.3490, 0.3333],\n",
      "          [0.4824, 0.4902, 0.4549,  ..., 0.3255, 0.3451, 0.3294],\n",
      "          [0.4196, 0.4588, 0.4392,  ..., 0.3412, 0.3373, 0.3333]]],\n",
      "\n",
      "\n",
      "        [[[0.4275, 0.4392, 0.4941,  ..., 0.0275, 0.0431, 0.0353],\n",
      "          [0.4667, 0.4745, 0.4078,  ..., 0.0667, 0.0745, 0.0784],\n",
      "          [0.4314, 0.3608, 0.3686,  ..., 0.1686, 0.2392, 0.1490],\n",
      "          ...,\n",
      "          [0.2863, 0.3765, 0.4275,  ..., 0.0863, 0.0706, 0.1294],\n",
      "          [0.2824, 0.3333, 0.4235,  ..., 0.0784, 0.0667, 0.1059],\n",
      "          [0.2471, 0.3020, 0.3176,  ..., 0.0667, 0.0549, 0.0824]]],\n",
      "\n",
      "\n",
      "        [[[0.9333, 0.9216, 0.7647,  ..., 0.4118, 0.2392, 0.5137],\n",
      "          [0.9020, 0.8980, 0.7176,  ..., 0.3608, 0.2980, 0.4078],\n",
      "          [0.9216, 0.8863, 0.9176,  ..., 0.1333, 0.1333, 0.1569],\n",
      "          ...,\n",
      "          [0.3529, 0.4078, 0.4157,  ..., 0.9255, 0.9412, 0.8902],\n",
      "          [0.4118, 0.4824, 0.4941,  ..., 0.9216, 0.9216, 0.9373],\n",
      "          [0.0941, 0.5020, 0.5098,  ..., 0.9098, 0.9098, 0.9255]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.5137, 0.3451, 0.2980,  ..., 0.8980, 0.9176, 0.8431],\n",
      "          [0.5843, 0.2431, 0.2667,  ..., 0.8745, 0.8392, 0.7098],\n",
      "          [0.5765, 0.2588, 0.2196,  ..., 0.8824, 0.8000, 0.6471],\n",
      "          ...,\n",
      "          [0.3843, 0.4863, 0.5412,  ..., 0.4118, 0.3451, 0.4039],\n",
      "          [0.6627, 0.5451, 0.4353,  ..., 0.3647, 0.4235, 0.2745],\n",
      "          [0.3098, 0.3333, 0.3176,  ..., 0.4549, 0.1294, 0.3882]]],\n",
      "\n",
      "\n",
      "        [[[0.7490, 0.8235, 0.8196,  ..., 0.3137, 0.2784, 0.0980],\n",
      "          [0.8078, 0.7961, 0.8196,  ..., 0.3176, 0.2863, 0.0824],\n",
      "          [0.6980, 0.7765, 0.8196,  ..., 0.3373, 0.2863, 0.0706],\n",
      "          ...,\n",
      "          [0.8353, 0.9020, 0.7020,  ..., 0.9961, 0.9882, 0.9922],\n",
      "          [0.8588, 0.9020, 0.7804,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9922, 0.9529, 0.9020,  ..., 0.9961, 0.9961, 0.9961]]],\n",
      "\n",
      "\n",
      "        [[[0.2471, 0.2431, 0.2431,  ..., 0.1569, 0.1294, 0.1569],\n",
      "          [0.2431, 0.2510, 0.2392,  ..., 0.1490, 0.1529, 0.1529],\n",
      "          [0.2392, 0.2235, 0.2196,  ..., 0.1490, 0.1490, 0.1569],\n",
      "          ...,\n",
      "          [0.5804, 0.5804, 0.5725,  ..., 0.1765, 0.1804, 0.1961],\n",
      "          [0.5490, 0.5529, 0.5843,  ..., 0.1765, 0.1843, 0.1843],\n",
      "          [0.5647, 0.5725, 0.5490,  ..., 0.2118, 0.1922, 0.1843]]]])\n",
      "tensor([[0.4932, 0.5068],\n",
      "        [0.4918, 0.5082],\n",
      "        [0.4883, 0.5117],\n",
      "        [0.4912, 0.5088],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.4891, 0.5109],\n",
      "        [0.4917, 0.5083],\n",
      "        [0.4891, 0.5109],\n",
      "        [0.4845, 0.5155],\n",
      "        [0.4916, 0.5084],\n",
      "        [0.4875, 0.5125],\n",
      "        [0.4897, 0.5103],\n",
      "        [0.4852, 0.5148],\n",
      "        [0.4896, 0.5104],\n",
      "        [0.4876, 0.5124],\n",
      "        [0.4905, 0.5095],\n",
      "        [0.4926, 0.5074],\n",
      "        [0.4891, 0.5109],\n",
      "        [0.4881, 0.5119],\n",
      "        [0.4852, 0.5148],\n",
      "        [0.4861, 0.5139],\n",
      "        [0.4875, 0.5125],\n",
      "        [0.4899, 0.5101],\n",
      "        [0.4894, 0.5106],\n",
      "        [0.4856, 0.5144],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.4903, 0.5097],\n",
      "        [0.4894, 0.5106],\n",
      "        [0.4857, 0.5143],\n",
      "        [0.4823, 0.5177],\n",
      "        [0.4934, 0.5066],\n",
      "        [0.4863, 0.5137],\n",
      "        [0.4958, 0.5042],\n",
      "        [0.4844, 0.5156],\n",
      "        [0.4910, 0.5090],\n",
      "        [0.4866, 0.5134],\n",
      "        [0.4906, 0.5094],\n",
      "        [0.4898, 0.5102],\n",
      "        [0.4865, 0.5135],\n",
      "        [0.4898, 0.5102],\n",
      "        [0.4887, 0.5113],\n",
      "        [0.4907, 0.5093],\n",
      "        [0.4949, 0.5051],\n",
      "        [0.4900, 0.5100],\n",
      "        [0.4918, 0.5082],\n",
      "        [0.4854, 0.5146],\n",
      "        [0.4907, 0.5093],\n",
      "        [0.4819, 0.5181],\n",
      "        [0.4935, 0.5065],\n",
      "        [0.5011, 0.4989],\n",
      "        [0.4878, 0.5122],\n",
      "        [0.4837, 0.5163],\n",
      "        [0.4904, 0.5096],\n",
      "        [0.4880, 0.5120],\n",
      "        [0.4930, 0.5070],\n",
      "        [0.4913, 0.5087],\n",
      "        [0.4883, 0.5117],\n",
      "        [0.4895, 0.5105],\n",
      "        [0.4886, 0.5114],\n",
      "        [0.4991, 0.5009],\n",
      "        [0.4840, 0.5160],\n",
      "        [0.4954, 0.5046],\n",
      "        [0.4864, 0.5136],\n",
      "        [0.4895, 0.5105],\n",
      "        [0.4891, 0.5109],\n",
      "        [0.4848, 0.5152],\n",
      "        [0.4887, 0.5113],\n",
      "        [0.4878, 0.5122],\n",
      "        [0.4915, 0.5085],\n",
      "        [0.4891, 0.5109],\n",
      "        [0.4935, 0.5065],\n",
      "        [0.4932, 0.5068],\n",
      "        [0.4910, 0.5090],\n",
      "        [0.4927, 0.5073],\n",
      "        [0.4921, 0.5079],\n",
      "        [0.4927, 0.5073],\n",
      "        [0.4879, 0.5121],\n",
      "        [0.4872, 0.5128],\n",
      "        [0.4886, 0.5114],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.4859, 0.5141],\n",
      "        [0.4910, 0.5090],\n",
      "        [0.4846, 0.5154],\n",
      "        [0.4922, 0.5078],\n",
      "        [0.4834, 0.5166],\n",
      "        [0.4883, 0.5117],\n",
      "        [0.4928, 0.5072],\n",
      "        [0.4913, 0.5087],\n",
      "        [0.4865, 0.5135],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.4904, 0.5096],\n",
      "        [0.4931, 0.5069],\n",
      "        [0.4927, 0.5073],\n",
      "        [0.4843, 0.5157],\n",
      "        [0.4874, 0.5126],\n",
      "        [0.4919, 0.5081],\n",
      "        [0.4935, 0.5065],\n",
      "        [0.4919, 0.5081],\n",
      "        [0.4857, 0.5143],\n",
      "        [0.4946, 0.5054]], grad_fn=<SoftmaxBackward0>) tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n",
      "Epoch: 0. Loss: 0.25002631545066833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [00:25<00:00,  8.75it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        net.zero_grad()\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        if i%10000==0:\n",
    "            print(batch_X)\n",
    "            print(outputs,batch_y)\n",
    "            print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055558b6-d709-411e-bd66-cc9b1a413723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2494/2494 [00:02<00:00, 958.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7eaf01-6b62-466f-9211-e57fe1ae13ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
